
"""
================================================================================
REAL NEWS INTEGRATION - Replace Mock Data with Actual News Sources
================================================================================

Integrates:
1. NewsAPI (https://newsapi.org) - Free tier: 100 requests/day
2. RSS Feeds (CNN, BBC, Reuters, AP, Guardian)
3. Reddit API (PRAW) - Real social trends
4. arXiv API - Real academic papers
5. GDELT API - Global news database (free)

Setup:
1. Get free API key from newsapi.org
2. Add to .env: NEWSAPI_KEY=your_key
3. Install: pip install newsapi-python feedparser praw arxiv
================================================================================
"""

import os
import json
import asyncio
import feedparser
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from urllib.parse import quote

import requests
from crewai.tools import BaseTool

# Try to import optional dependencies
try:
    from newsapi import NewsApiClient
    NEWSAPI_AVAILABLE = True
except ImportError:
    NEWSAPI_AVAILABLE = False

try:
    import praw
    REDDIT_AVAILABLE = True
except ImportError:
    REDDIT_AVAILABLE = False

try:
    import arxiv
    ARXIV_AVAILABLE = True
except ImportError:
    ARXIV_AVAILABLE = False

# ==============================================================================
# REAL NEWS API TOOLS
# ==============================================================================

class RealNewsSearchTool(BaseTool):
    """
    Search REAL news using NewsAPI + RSS feeds.
    Falls back to RSS if NewsAPI rate limit hit.
    """
    name: str = "real_news_search"
    description: str = "Search for real news articles from NewsAPI and major RSS feeds"

    def __init__(self):
        super().__init__()
        self.newsapi_key = os.getenv("NEWSAPI_KEY")
        self.newsapi = NewsApiClient(api_key=self.newsapi_key) if NEWSAPI_AVAILABLE and self.newsapi_key else None

        # Major news RSS feeds
        self.rss_feeds = {
            "bbc": "http://feeds.bbci.co.uk/news/world/rss.xml",
            "cnn": "http://rss.cnn.com/rss/edition.rss",
            "reuters": "http://feeds.reuters.com/reuters/topNews",
            "guardian": "https://www.theguardian.com/world/rss",
            "aljazeera": "https://www.aljazeera.com/xml/rss/all.xml",
            "ap": "https://rsshub.app/apnews/topics/ap-top-news",
            "techcrunch": "https://techcrunch.com/feed/",
            "verge": "https://www.theverge.com/rss/index.xml",
            "wired": "https://www.wired.com/feed/rss",
            "economist": "https://www.economist.com/latest/rss.xml"
        }

    def _run(self, query: str, days_back: int = 3) -> str:
        """Search for real news articles."""

        results = []

        # Try NewsAPI first (if available)
        if self.newsapi:
            try:
                newsapi_results = self._search_newsapi(query, days_back)
                results.extend(newsapi_results)
                print(f"    üì∞ NewsAPI: Found {len(newsapi_results)} articles")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  NewsAPI error: {e}")

        # Fallback to RSS feeds
        try:
            rss_results = self._search_rss(query, days_back)
            results.extend(rss_results)
            print(f"    üì° RSS Feeds: Found {len(rss_results)} articles")
        except Exception as e:
            print(f"    ‚ö†Ô∏è  RSS error: {e}")

        # Remove duplicates by URL
        seen_urls = set()
        unique_results = []
        for r in results:
            url = r.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(r)

        if unique_results:
            print(f"    ‚úÖ Total unique articles: {len(unique_results)}")
            return json.dumps(unique_results[:10])  # Return top 10
        else:
            print(f"    ‚ö†Ô∏è  No real news found, using emergency fallback")
            return self._emergency_fallback(query)

    def _search_newsapi(self, query: str, days_back: int) -> List[Dict]:
        """Search using NewsAPI."""
        if not self.newsapi:
            return []

        from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')

        response = self.newsapi.get_everything(
            q=query,
            from_param=from_date,
            language='en',
            sort_by='relevancy',
            page_size=10
        )

        articles = []
        for article in response.get('articles', []):
            articles.append({
                "title": article.get('title', ''),
                "url": article.get('url', ''),
                "body": article.get('description', article.get('content', ''))[:300],
                "source": article.get('source', {}).get('name', 'Unknown'),
                "published_at": article.get('publishedAt', ''),
                "reliability": "tier_1_verified" if article.get('source', {}).get('name') in ['Reuters', 'Associated Press', 'BBC News'] else "tier_2_reputable"
            })

        return articles

    def _search_rss(self, query: str, days_back: int) -> List[Dict]:
        """Search using RSS feeds."""
        results = []
        query_lower = query.lower()

        # Select feeds based on query category
        feeds_to_check = list(self.rss_feeds.items())

        # Prioritize tech feeds for tech topics
        if any(word in query_lower for word in ['tech', 'ai', 'artificial intelligence', 'software', 'app']):
            feeds_to_check = [
                ('techcrunch', self.rss_feeds['techcrunch']),
                ('verge', self.rss_feeds['verge']),
                ('wired', self.rss_feeds['wired']),
                ('bbc', self.rss_feeds['bbc']),
                ('guardian', self.rss_feeds['guardian'])
            ]

        # Check first 5 feeds (to avoid timeout)
        for feed_name, feed_url in feeds_to_check[:5]:
            try:
                feed = feedparser.parse(feed_url)

                for entry in feed.entries[:5]:  # Top 5 from each feed
                    title = entry.get('title', '')
                    summary = entry.get('summary', entry.get('description', ''))

                    # Check if query matches
                    if query_lower in title.lower() or query_lower in summary.lower() or any(word in title.lower() for word in query_lower.split()):
                        results.append({
                            "title": title,
                            "url": entry.get('link', ''),
                            "body": summary[:300],
                            "source": feed_name.upper(),
                            "published_at": entry.get('published', ''),
                            "reliability": "tier_1_verified" if feed_name in ['reuters', 'ap', 'bbc'] else "tier_2_reputable"
                        })

            except Exception as e:
                continue  # Skip failed feeds

        return results

    def _emergency_fallback(self, query: str) -> str:
        """Last resort: return helpful message."""
        return json.dumps([{
            "title": f"Live Update: {query.title()} Developments",
            "url": f"https://news.google.com/search?q={quote(query)}",
            "body": f"Search Google News for latest updates on {query}. No API results available - check direct sources.",
            "source": "Google News (Manual Search Required)",
            "reliability": "tier_3_emerging"
        }])

class RealSocialMediaTool(BaseTool):
    """
    Get REAL social media trends using Reddit API.
    (Twitter/X requires expensive API, so we use Reddit + mock for others)
    """
    name: str = "real_social_monitor"
    description: str = "Monitor real social media trends from Reddit"

    def __init__(self):
        super().__init__()
        self.reddit = None

        # Try to initialize Reddit
        if REDDIT_AVAILABLE:
            try:
                self.reddit = praw.Reddit(
                    client_id=os.getenv("REDDIT_CLIENT_ID"),
                    client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
                    user_agent="NewsReels/1.0 by YourUsername"
                )
                print("    ‚úÖ Reddit API connected")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Reddit not configured: {e}")

    def _run(self, platform: str, topic: str) -> str:
        """Get real social media data."""

        results = []

        # Reddit (real data)
        if platform.lower() in ['reddit', 'all'] and self.reddit:
            try:
                reddit_results = self._search_reddit(topic)
                results.extend(reddit_results)
                print(f"    üì± Reddit: Found {len(reddit_results)} trending posts")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Reddit error: {e}")

        # Twitter/X simulation (since API is expensive)
        if platform.lower() in ['twitter', 'all']:
            # Use Google Trends or simulate based on Reddit data
            twitter_results = self._simulate_twitter_trends(topic, len(results))
            results.extend(twitter_results)
            print(f"    üì± Twitter trends: Generated {len(twitter_results)} trends")

        if results:
            return json.dumps(results)
        else:
            return self._fallback_mock(topic)

    def _search_reddit(self, topic: str) -> List[Dict]:
        """Search Reddit for real trending posts."""
        results = []

        # Search relevant subreddits
        subreddits = ['news', 'worldnews', 'technology', 'science', 'politics']

        for subreddit_name in subreddits:
            try:
                subreddit = self.reddit.subreddit(subreddit_name)

                # Search top posts from last 24 hours
                for post in subreddit.search(topic, sort='hot', time_filter='day', limit=3):
                    if post.score > 50:  # Only popular posts
                        results.append({
                            "headline": post.title,
                            "platform": "Reddit",
                            "engagement": f"{post.score} upvotes, {post.num_comments} comments",
                            "subreddit": f"r/{subreddit_name}",
                            "sentiment": "positive" if post.upvote_ratio > 0.7 else "mixed" if post.upvote_ratio > 0.5 else "controversial",
                            "confidence": min(post.upvote_ratio, 0.9),
                            "urgency": 0.7 if post.score > 1000 else 0.5,
                            "novelty": 0.8,
                            "url": f"https://reddit.com{post.permalink}"
                        })

            except Exception as e:
                continue

        return results[:5]  # Top 5 results

    def _simulate_twitter_trends(self, topic: str, reddit_count: int) -> List[Dict]:
        """Simulate Twitter trends based on topic popularity."""
        # In production, use Google Trends API or Twitter scraping
        return [{
            "headline": f"Twitter Trend: {topic.title()} gaining traction",
            "platform": "Twitter",
            "engagement": f"{reddit_count * 1000} tweets, {reddit_count * 500} retweets",
            "influencers": [f"@{topic.replace(' ', '')}_news", "@breaking_updates"],
            "sentiment": "concerned",
            "confidence": 0.75,
            "urgency": 0.7,
            "novelty": 0.8
        }]

    def _fallback_mock(self, topic: str) -> str:
        """Fallback if no real data available."""
        return json.dumps([{
            "headline": f"Trending Discussion: {topic.title()}",
            "platform": "Social Media",
            "engagement": "High engagement detected",
            "sentiment": "mixed",
            "confidence": 0.6,
            "urgency": 0.5,
            "novelty": 0.7
        }])

class RealAcademicSearchTool(BaseTool):
    """
    Search REAL academic papers using arXiv API.
    """
    name: str = "real_academic_search"
    description: str = "Search real academic papers from arXiv"

    def _run(self, query: str, source_type: str = "all") -> str:
        """Search arXiv for real papers."""

        if not ARXIV_AVAILABLE:
            return json.dumps([])

        try:
            # Search arXiv
            search = arxiv.Search(
                query=query,
                max_results=5,
                sort_by=arxiv.SortCriterion.SubmittedDate
            )

            results = []
            for paper in search.results():
                results.append({
                    "title": paper.title,
                    "url": paper.pdf_url,
                    "body": paper.summary[:300],
                    "source": "arXiv",
                    "published_at": paper.published.isoformat(),
                    "reliability": "tier_1_verified",
                    "authors": [str(a) for a in paper.authors[:3]]
                })

            print(f"    üìö arXiv: Found {len(results)} papers")
            return json.dumps(results)

        except Exception as e:
            print(f"    ‚ö†Ô∏è  arXiv error: {e}")
            return json.dumps([])

# ==============================================================================
# INTEGRATION - Replace Tools in scout_council_swarm.py
# ==============================================================================

def get_real_tools():
    """Return tools that use real APIs instead of mock data."""
    return {
        "news_search": RealNewsSearchTool(),
        "social_media_monitor": RealSocialMediaTool(),
        "academic_search": RealAcademicSearchTool()
    }

# ==============================================================================
# SETUP INSTRUCTIONS
# ==============================================================================

SETUP_INSTRUCTIONS = """
SETUP FOR REAL NEWS:
====================

1. INSTALL DEPENDENCIES:
   pip install newsapi-python feedparser praw arxiv

2. GET API KEYS:

   a) NewsAPI (FREE - 100 requests/day):
      - Go to: https://newsapi.org/
      - Sign up and get API key
      - Add to .env: NEWSAPI_KEY=your_key_here

   b) Reddit API (FREE):
      - Go to: https://www.reddit.com/prefs/apps
      - Create app (script type)
      - Add to .env:
        REDDIT_CLIENT_ID=your_id
        REDDIT_CLIENT_SECRET=your_secret

   c) arXiv (FREE - no key needed):
      - Just install the package, no key required

3. UPDATE scout_council_swarm.py:

   REPLACE the tool initialization:

   OLD:
       @tool("news_search")
       def news_search(...)

   NEW:
       from real_news_integration import get_real_tools

       # In your agent creation:
       tools = get_real_tools()
       wire_scout = Agent(
           ...,
           tools=[tools["news_search"], tools["social_media_monitor"]]
       )

4. RUN:
   python example_usage.py

   You should see:
   üì∞ NewsAPI: Found X articles
   üì° RSS Feeds: Found Y articles
   üì± Reddit: Found Z trending posts
   üìö arXiv: Found W papers

5. TROUBLESHOOTING:

   If NewsAPI rate limit (100/day) exceeded:
   - System automatically falls back to RSS feeds
   - Or wait 24 hours for reset

   If Reddit not working:
   - Check credentials in .env
   - Or system uses fallback trends

   If no results:
   - Check internet connection
   - Try broader search terms
   - Check API keys are loaded: python -c "import os; print(os.getenv('NEWSAPI_KEY'))"

EXPECTED OUTPUT:
===============

============================================================
PHASE 1: SCOUT DISCOVERY
============================================================
  ‚Üí Launching Wire Scout - Breaking News Hunter...
    üì∞ NewsAPI: Found 5 articles
    üì° RSS Feeds: Found 3 articles
    ‚úÖ Total unique articles: 8
    ‚úÖ Wire Scout found 3 stories

  ‚Üí Launching Social Scout - Viral Content Detector...
    üì± Reddit: Found 4 trending posts
    üì± Twitter trends: Generated 2 trends
    ‚úÖ Social Scout found 2 stories

  ‚Üí Launching Semantic Scout - Deep Research Specialist...
    üìö arXiv: Found 3 papers
    ‚úÖ Semantic Scout found 1 stories

‚úì Discovery complete: 6 unique stories found

Stories will be from REAL sources like:
- Reuters, BBC, CNN, Guardian
- Reddit discussions
- arXiv research papers
"""

print(SETUP_INSTRUCTIONS)